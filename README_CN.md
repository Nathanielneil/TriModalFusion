# TriModalFusion: ç»Ÿä¸€å¤šæ¨¡æ€è¯†åˆ«ç³»ç»Ÿ

<p align="right">
<strong>è¯­è¨€</strong>: <a href="README.md">English</a> | <a href="README_CN.md">ä¸­æ–‡</a>
</p>

<div align="center">

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-orange.svg)](https://pytorch.org/)
[![Lightning](https://img.shields.io/badge/Lightning-2.0+-purple.svg)](https://lightning.ai/)
[![Transformers](https://img.shields.io/badge/ğŸ¤—%20Transformers-4.30+-yellow.svg)](https://huggingface.co/transformers/)
[![OpenCV](https://img.shields.io/badge/OpenCV-4.8+-green.svg)](https://opencv.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-brightgreen.svg)](https://opensource.org/licenses/MIT)

[![Speech Recognition](https://img.shields.io/badge/Speech-Recognition-ff6b6b.svg)](#)
[![Gesture Recognition](https://img.shields.io/badge/Gesture-Recognition-4ecdc4.svg)](#)
[![Computer Vision](https://img.shields.io/badge/Computer-Vision-45b7d1.svg)](#)
[![Multimodal AI](https://img.shields.io/badge/Multimodal-AI-6c5ce7.svg)](#)
[![Docker](https://img.shields.io/badge/Docker-Ready-0db7ed.svg)](https://www.docker.com/)
[![Kubernetes](https://img.shields.io/badge/Kubernetes-Ready-326CE5.svg)](https://kubernetes.io/)
[![CUDA](https://img.shields.io/badge/CUDA-Ready-76B900.svg)](https://developer.nvidia.com/cuda-toolkit)

</div>

TriModalFusionæ˜¯ä¸€ä¸ªé›†æˆè¯­éŸ³ã€æ‰‹åŠ¿å’Œå›¾åƒå¤„ç†èƒ½åŠ›çš„æ·±åº¦å­¦ä¹ å¤šæ¨¡æ€è¯†åˆ«æ¡†æ¶ã€‚è¯¥ç³»ç»Ÿå®ç°äº†åŒ…æ‹¬Transformerç½‘ç»œã€MediaPipeæ‰‹éƒ¨è¿½è¸ªå’Œè§†è§‰Transformeråœ¨å†…çš„æœ€æ–°æ¶æ„ï¼Œä¸ºå¤šæ¨¡æ€äººå·¥æ™ºèƒ½åº”ç”¨æä¾›ç»Ÿä¸€çš„è®¡ç®—å¹³å°ã€‚

## æ ¸å¿ƒç‰¹æ€§

### å¤šæ¨¡æ€å¤„ç†
- **è¯­éŸ³è¯†åˆ«**: åŸºäºTransformerçš„ç¼–ç å™¨æ¶æ„ï¼Œç”¨äºéŸ³é¢‘åºåˆ—å¤„ç†
- **æ‰‹åŠ¿è¯†åˆ«**: MediaPipeæ‰‹éƒ¨è¿½è¸ªä¸å›¾å·ç§¯ç½‘ç»œç›¸ç»“åˆï¼Œè¿›è¡Œæ—¶ç©ºæ‰‹åŠ¿åˆ†æ
- **å›¾åƒè¯†åˆ«**: è§†è§‰Transformerå’Œå·ç§¯ç¥ç»ç½‘ç»œæ¶æ„ï¼Œæ”¯æŒå¯é€‰çš„ç›®æ ‡æ£€æµ‹èƒ½åŠ›
- **è·¨æ¨¡æ€èåˆ**: å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å®ç°è·¨æ¨¡æ€ç‰¹å¾é›†æˆ

### è·¨æ¨¡æ€èåˆæœºåˆ¶
- **æ—¶åºå¯¹é½**: é’ˆå¯¹å˜é•¿åºåˆ—çš„å¤šå°ºåº¦æ—¶åºåŒæ­¥
- **è¯­ä¹‰å¯¹é½**: é€šè¿‡å¯¹æ¯”å­¦ä¹ æ–¹æ³•æ„å»ºç»Ÿä¸€çš„è·¨æ¨¡æ€è¡¨ç¤ºç©ºé—´
- **å±‚æ¬¡åŒ–èåˆ**: åœ¨ç‰¹å¾ã€è¯­ä¹‰å’Œå†³ç­–æŠ½è±¡å±‚é¢çš„æ¸è¿›ä¿¡æ¯é›†æˆ
- **è·¨æ¨¡æ€æ³¨æ„åŠ›**: æ¨¡æ€è¡¨ç¤ºé—´çš„åŒå‘æ³¨æ„åŠ›è®¡ç®—

### ç³»ç»Ÿæ¶æ„
- **æ¨¡å—åŒ–è®¾è®¡**: åŸºäºç»„ä»¶çš„æ¶æ„ï¼Œæ”¯æŒå¯æ‰©å±•çš„æ¨¡æ€é›†æˆ
- **å¤šä»»åŠ¡å­¦ä¹ **: æ”¯æŒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å›å½’å’Œåºåˆ—ç”Ÿæˆä»»åŠ¡
- **é…ç½®ç®¡ç†**: åŸºäºYAMLçš„å‚æ•°é…ç½®ç³»ç»Ÿ
- **è¯„ä¼°æ¡†æ¶**: ç»¼åˆæŒ‡æ ‡è®¡ç®—å’Œæ¨¡å‹æ£€æŸ¥ç‚¹èƒ½åŠ›

## å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
git clone https://github.com/Nathanielneil/TriModalFusion.git
cd TriModalFusion
pip install -r requirements.txt
```

### åŸºæœ¬ä½¿ç”¨

```python
import torch
from src.models.trimodal_fusion import TriModalFusionModel
from src.utils.config import load_config

# åŠ è½½é…ç½®
config = load_config("configs/default_config.yaml")

# åˆ›å»ºæ¨¡å‹
model = TriModalFusionModel(config)

# å‡†å¤‡å¤šæ¨¡æ€è¾“å…¥
inputs = {
    'speech': torch.randn(2, 16000),          # éŸ³é¢‘: [æ‰¹æ¬¡, é‡‡æ ·ç‚¹]
    'gesture': torch.randn(2, 30, 2, 21, 3), # å…³é”®ç‚¹: [æ‰¹æ¬¡, æ—¶é—´, æ‰‹æ•°, å…³èŠ‚, åæ ‡]
    'image': torch.randn(2, 3, 224, 224)     # å›¾åƒ: [æ‰¹æ¬¡, é€šé“, é«˜åº¦, å®½åº¦]
}

# å‰å‘ä¼ æ’­
outputs = model(inputs)
predictions = outputs['task_outputs']['classification']
```

### è®­ç»ƒç¤ºä¾‹

```python
# è®¾ç½®è®­ç»ƒ
model.train()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

# è®­ç»ƒæ­¥éª¤
outputs = model(inputs)
targets = {'classification': torch.randint(0, 10, (2,))}
losses = model.compute_loss(outputs, targets)

losses['total_loss'].backward()
optimizer.step()
optimizer.zero_grad()
```

## æ¶æ„æ¦‚è§ˆ

### ç³»ç»Ÿæ¶æ„

```mermaid
graph TB
    subgraph INPUT ["è¾“å…¥å±‚"]
        A1["è¯­éŸ³<br/>16kHzéŸ³é¢‘"]
        A2["æ‰‹åŠ¿<br/>å…³é”®ç‚¹"]
        A3["å›¾åƒ<br/>224Ã—224"]
    end
    
    subgraph ENCODE ["ç¼–ç å±‚"]
        B1["è¯­éŸ³<br/>Transformer"]
        B2["æ‰‹åŠ¿<br/>GCN+CNN"]
        B3["å›¾åƒ<br/>ViT/CNN"]
    end
    
    subgraph FUSION ["èåˆå±‚"]
        C1["æ—¶åº<br/>å¯¹é½"]
        C2["è¯­ä¹‰<br/>å¯¹é½"]
        C3["è·¨æ¨¡æ€<br/>æ³¨æ„åŠ›"]
    end
    
    subgraph INTEGRATE ["é›†æˆå±‚"]
        D1["ç‰¹å¾èåˆ"]
        D2["è¯­ä¹‰èåˆ"]
        D3["å†³ç­–èåˆ"]
    end
    
    subgraph OUTPUT ["è¾“å‡ºå±‚"]
        E1["åˆ†ç±»"]
        E2["æ£€æµ‹"]
        E3["å›å½’"]
    end
    
    A1 --> B1
    A2 --> B2
    A3 --> B3
    
    B1 --> C1
    B2 --> C1
    B3 --> C1
    
    B1 --> C2
    B2 --> C2
    B3 --> C2
    
    B1 --> C3
    B2 --> C3
    B3 --> C3
    
    C1 --> D1
    C2 --> D2
    C3 --> D3
    
    D1 --> E1
    D2 --> E1
    D3 --> E1
    
    D1 --> E2
    D2 --> E2
    D3 --> E2
    
    D1 --> E3
    D2 --> E3
    D3 --> E3
    
    classDef inputStyle fill:#e1f5fe,stroke:#0288d1,stroke-width:3px,font-size:14px,font-weight:bold
    classDef encoderStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px,font-size:14px,font-weight:bold
    classDef fusionStyle fill:#e8f5e8,stroke:#388e3c,stroke-width:3px,font-size:14px,font-weight:bold
    classDef integrationStyle fill:#fff3e0,stroke:#f57c00,stroke-width:3px,font-size:14px,font-weight:bold
    classDef outputStyle fill:#fce4ec,stroke:#c2185b,stroke-width:3px,font-size:14px,font-weight:bold
    
    class A1,A2,A3 inputStyle
    class B1,B2,B3 encoderStyle
    class C1,C2,C3 fusionStyle
    class D1,D2,D3 integrationStyle
    class E1,E2,E3 outputStyle
```

### æ•°æ®æµæ¶æ„

```mermaid
flowchart LR
    subgraph INPUTS ["è¾“å…¥æ¨¡æ€"]
        A["è¯­éŸ³<br/>æ³¢å½¢"]
        B["æ‰‹åŠ¿<br/>å…³é”®ç‚¹"]
        C["å›¾åƒ<br/>åƒç´ "]
    end
    
    subgraph ENCODERS ["æ¨¡æ€ç¼–ç å™¨"]
        D["TRANSFORMER<br/>ç¼–ç å™¨"]
        E["GCN+CNN<br/>ç¼–ç å™¨"]
        F["ViT/CNN<br/>ç¼–ç å™¨"]
    end
    
    subgraph FUSION ["è·¨æ¨¡æ€èåˆ"]
        G["å¤šå¤´<br/>æ³¨æ„åŠ›"]
    end
    
    subgraph OUTPUTS ["ä»»åŠ¡è¾“å‡º"]
        H["åˆ†ç±»<br/>å¤´"]
        I["æ£€æµ‹<br/>å¤´"]
        J["å›å½’<br/>å¤´"]
    end
    
    A --> D
    B --> E
    C --> F
    
    D --> G
    E --> G
    F --> G
    
    G --> H
    G --> I
    G --> J
    
    style INPUTS fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000,font-size:16px,font-weight:bold
    style ENCODERS fill:#f1f8e9,stroke:#388e3c,stroke-width:3px,color:#000,font-size:16px,font-weight:bold
    style FUSION fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000,font-size:16px,font-weight:bold
    style OUTPUTS fill:#fce4ec,stroke:#c2185b,stroke-width:3px,color:#000,font-size:16px,font-weight:bold
    
    classDef nodeStyle fill:#ffffff,stroke:#333,stroke-width:3px,font-size:15px,font-weight:bold
    class A,B,C,D,E,F,G,H,I,J nodeStyle
```

### ç»„ä»¶è¯¦ç»†è¯´æ˜

#### è¯­éŸ³ç¼–ç å™¨
- **æ¢…å°”é¢‘è°±å›¾æå–**: 80é€šé“æ¢…å°”é¢‘ç‡å€’è°±ç³»æ•°è®¡ç®—
- **Transformeræ¶æ„**: å…·æœ‰å­¦ä¹ ä½ç½®ç¼–ç çš„å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚
- **ç‰¹å¾èšåˆ**: å¯é…ç½®çš„æ± åŒ–ç­–ç•¥ï¼ŒåŒ…æ‹¬å‡å€¼ã€æœ€å¤§å€¼ã€æ³¨æ„åŠ›åŠ æƒå’Œåˆ†ç±»ä»¤ç‰Œæ± åŒ–

#### æ‰‹åŠ¿ç¼–ç å™¨
- **æ‰‹éƒ¨å…³é”®ç‚¹æ£€æµ‹**: MediaPipeæ¡†æ¶å®ç°å®æ—¶21ä¸ªåœ°æ ‡ç‚¹æ‰‹éƒ¨å§¿æ€ä¼°è®¡
- **å›¾å·ç§¯ç½‘ç»œ**: æ‰‹éƒ¨éª¨éª¼ç»“æ„çš„ç©ºé—´å…³ç³»å»ºæ¨¡
- **æ—¶åºå·ç§¯ç½‘ç»œ**: æ‰‹åŠ¿åºåˆ—ä¸­çš„å¤šå°ºåº¦æ—¶åºæ¨¡å¼è¯†åˆ«

#### å›¾åƒç¼–ç å™¨
- **è§†è§‰Transformer**: åŸºäºå—çš„å›¾åƒä»¤ç‰ŒåŒ–ä¸å¤šå¤´è‡ªæ³¨æ„åŠ›è®¡ç®—
- **å·ç§¯ç¥ç»ç½‘ç»œ**: æ”¯æŒResNetå’ŒEfficientNetä¸»å¹²æ¶æ„
- **ç›®æ ‡æ£€æµ‹**: DETRé£æ ¼çš„æ£€æµ‹å¤´ï¼Œç”¨äºè¾¹ç•Œæ¡†å›å½’å’Œåˆ†ç±»

#### èåˆæœºåˆ¶
- **æ—¶åºå¯¹é½**: åŸºäºæ’å€¼ã€æ³¨æ„åŠ›åŠ æƒå’Œå¯å­¦ä¹ çš„æ—¶åºåŒæ­¥æ–¹æ³•
- **è¯­ä¹‰å¯¹é½**: ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦å’ŒåŒçº¿æ€§å˜æ¢çš„å¯¹æ¯”å­¦ä¹ ä¼˜åŒ–
- **è·¨æ¨¡æ€æ³¨æ„åŠ›**: è·¨æ¨¡æ€ç‰¹å¾è¡¨ç¤ºçš„å¤šå¤´æ³¨æ„åŠ›è®¡ç®—
- **å±‚æ¬¡åŒ–é›†æˆ**: å¤šä¸ªæŠ½è±¡å±‚é¢çš„æ¸è¿›ä¿¡æ¯èåˆ

## æ€§èƒ½æŒ‡æ ‡

è¯¥æ¡†æ¶å®ç°äº†å…¨é¢çš„å®šé‡è¯„ä¼°æŒ‡æ ‡ï¼š

### è¯­éŸ³è¯†åˆ«æŒ‡æ ‡
- **è¯é”™è¯¯ç‡(WER)**: æ ‡å‡†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å‡†ç¡®ç‡æŒ‡æ ‡
- **å­—ç¬¦é”™è¯¯ç‡(CER)**: å­—ç¬¦çº§è½¬å½•å‡†ç¡®ç‡æµ‹é‡
- **BLEUåˆ†æ•°**: åŒè¯­è¯„ä¼°æ›¿ä»£åˆ†æ•°ï¼Œç”¨äºæ–‡æœ¬ç›¸ä¼¼åº¦è¯„ä¼°

### æ‰‹åŠ¿è¯†åˆ«æŒ‡æ ‡
- **åˆ†ç±»å‡†ç¡®ç‡**: å¤šç±»åˆ«æ‰‹åŠ¿è¯†åˆ«å‡†ç¡®ç‡
- **å…³é”®ç‚¹å®šä½è¯¯å·®**: å…³é”®ç‚¹åæ ‡é¢„æµ‹çš„å‡æ–¹è¯¯å·®å’Œå¹³å‡ç»å¯¹è¯¯å·®
- **æ—¶åºä¸€è‡´æ€§**: æ‰‹åŠ¿åºåˆ—é¢„æµ‹çš„æ—¶åºå¹³æ»‘åº¦æµ‹é‡

### å›¾åƒè¯†åˆ«æŒ‡æ ‡
- **Top-1/Top-5å‡†ç¡®ç‡**: æ ‡å‡†å¤šç±»åˆ«å›¾åƒåˆ†ç±»å‡†ç¡®ç‡æŒ‡æ ‡
- **å¹³å‡ç²¾åº¦å‡å€¼(mAP)**: å¤šä¸ªIoUé˜ˆå€¼ä¸‹çš„ç›®æ ‡æ£€æµ‹æ€§èƒ½è¯„ä¼°
- **äº¤å¹¶æ¯”åˆ†æ**: è¾¹ç•Œæ¡†å®šä½è´¨é‡è¯„ä¼°

### å¤šæ¨¡æ€èåˆæŒ‡æ ‡
- **æ¨¡æ€è´¡çŒ®åˆ†æ**: é€šè¿‡æ¶ˆèç ”ç©¶å®šé‡è¯„ä¼°å•ä¸ªæ¨¡æ€é‡è¦æ€§
- **èåˆæœ‰æ•ˆæ€§**: ç›¸å¯¹äºå•æ¨¡æ€åŸºå‡†çš„æ€§èƒ½æå‡é‡åŒ–
- **è·¨æ¨¡æ€ç›¸å…³æ€§**: æ¨¡æ€é—´ç‰¹å¾ç›¸ä¼¼åº¦å’Œå¯¹é½åº¦æµ‹é‡

## é…ç½®

ç³»ç»Ÿä½¿ç”¨YAMLé…ç½®æ–‡ä»¶ä»¥ä¾¿äºè‡ªå®šä¹‰ï¼š

```yaml
model:
  d_model: 512
  tasks: ["classification"]
  num_classes: 10

speech_config:
  sample_rate: 16000
  n_mels: 80
  pooling: "attention"

gesture_config:
  num_hands: 2
  use_mediapipe: true
  spatial_aggregation: "attention"

image_config:
  image_architecture: "vit"
  img_size: 224

fusion_config:
  fusion_strategy: "attention"
  alignment_method: "interpolation"

training:
  optimizer: "adamw"
  learning_rate: 1e-4
  batch_size: 32
```

## ç¤ºä¾‹

### å®é™…åº”ç”¨

```python
# ç¤ºä¾‹1: æ‰‹åŠ¿æ§åˆ¶ç•Œé¢
inputs = {
    'gesture': extract_keypoints_from_video(video_frames),
    'speech': record_audio_command()
}
action = model.inference(inputs, task='classification')

# ç¤ºä¾‹2: å¤šæ¨¡æ€å†…å®¹åˆ†æ
inputs = {
    'image': load_image("scene.jpg"),
    'speech': transcribe_audio("description.wav"),
    'gesture': detect_pointing_gesture(video_frames)
}
understanding = model.extract_features(inputs)

# ç¤ºä¾‹3: æ— éšœç¢åº”ç”¨
inputs = {
    'gesture': sign_language_video,
    'speech': spoken_description,
    'image': context_image
}
translation = model.inference(inputs, task='generation')
```

### è‡ªå®šä¹‰æ¨¡å‹è®­ç»ƒ

```python
# è‡ªå®šä¹‰é…ç½®
config = {
    'model': {'d_model': 256, 'tasks': ['classification']},
    'speech_config': {'pooling': 'cls'},
    'gesture_config': {'spatial_aggregation': 'attention'},
    'image_config': {'image_architecture': 'cnn'},
    'fusion_config': {'fusion_strategy': 'adaptive'}
}

# åˆå§‹åŒ–æ¨¡å‹
model = TriModalFusionModel(config)

# è‡ªå®šä¹‰æ•°æ®é›†
class MultimodalDataset(torch.utils.data.Dataset):
    def __init__(self, data_path):
        # åŠ è½½æ‚¨çš„å¤šæ¨¡æ€æ•°æ®é›†
        pass
    
    def __getitem__(self, idx):
        # è¿”å› {'speech': ..., 'gesture': ..., 'image': ...}, targets
        pass

# è®­ç»ƒå¾ªç¯
dataset = MultimodalDataset("path/to/data")
dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)

for batch in dataloader:
    inputs, targets = batch
    outputs = model(inputs)
    losses = model.compute_loss(outputs, targets)
    losses['total_loss'].backward()
    optimizer.step()
    optimizer.zero_grad()
```

## ç ”ç©¶åº”ç”¨

### ç ”ç©¶åº”ç”¨
- **äººæœºäº¤äº’**: å¼€å‘è‡ªç„¶å¤šæ¨¡æ€ç”¨æˆ·ç•Œé¢
- **æ— éšœç¢æŠ€æœ¯**: è‡ªåŠ¨æ‰‹è¯­è¯†åˆ«å’Œç¿»è¯‘ç³»ç»Ÿ
- **è¡Œä¸ºåˆ†æ**: äººç±»äº¤æµæ¨¡å¼çš„è®¡ç®—åˆ†æ
- **æœºå™¨äººå­¦**: äººæœºäº¤äº’çš„å¤šæ¨¡æ€æ„ŸçŸ¥ç³»ç»Ÿ
- **ä¸´åºŠè¯„ä¼°**: é€šè¿‡å¤šæ¨¡æ€è¡Œä¸ºåˆ†æè¿›è¡ŒåŒ»ç–—è¯„ä¼°

### ç ”ç©¶ç‰¹æ€§
- **å¯é‡ç°å®éªŒ**: å…¨é¢çš„é…ç½®ç®¡ç†å’Œå®éªŒè®°å½•
- **æ¶ˆèåˆ†æ**: æ¨¡å—åŒ–æ¶æ„æ”¯æŒç³»ç»Ÿç»„ä»¶è¯„ä¼°
- **åŸºå‡†è¯„ä¼°**: æ ‡å‡†åŒ–è¯„ä¼°åè®®ç”¨äºç ”ç©¶æ¯”è¾ƒ
- **æ¶æ„å¯æ‰©å±•æ€§**: æ¡†æ¶è®¾è®¡æ”¯æŒæ–°æ¨¡æ€å’Œæ¶æ„çš„é›†æˆ

## æ–‡æ¡£

### APIæ–‡æ¡£
- [æ¨¡å‹æ¶æ„](docs/model_architecture.md)
- [é…ç½®æŒ‡å—](docs/configuration.md)
- [è®­ç»ƒæŒ‡å—](docs/training.md)
- [è¯„ä¼°æŒ‡æ ‡](docs/evaluation.md)

### æ•™ç¨‹
- [å…¥é—¨æŒ‡å—](docs/getting_started.md)
- [è‡ªå®šä¹‰æ•°æ®é›†](docs/custom_datasets.md)
- [é«˜çº§é…ç½®](docs/advanced_config.md)
- [éƒ¨ç½²æŒ‡å—](docs/deployment.md)

## è´¡çŒ®

æˆ‘ä»¬æ¬¢è¿è´¡çŒ®ï¼è¯·æŸ¥çœ‹[CONTRIBUTING.md](CONTRIBUTING.md)äº†è§£æŒ‡å—ã€‚

### å¼€å‘ç¯å¢ƒè®¾ç½®
```bash
# å…‹éš†ä»“åº“
git clone https://github.com/Nathanielneil/TriModalFusion.git
cd TriModalFusion

# å®‰è£…å¼€å‘ä¾èµ–
pip install -r requirements-dev.txt

# è¿è¡Œæµ‹è¯•
python -m pytest tests/

# ä»£ç æ ¼å¼åŒ–
black src/ tests/
flake8 src/ tests/
```

## è®¸å¯è¯

è¯¥é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ - è¯¦æƒ…è¯·æŸ¥çœ‹[LICENSE](LICENSE)æ–‡ä»¶ã€‚

## è‡´è°¢

- **MediaPipe**: è°·æ­Œç ”ç©¶çš„å¤šæ¨¡æ€æ„ŸçŸ¥ç®¡é“æ¡†æ¶
- **Whisper**: OpenAIé€šè¿‡å¤§è§„æ¨¡å¼±ç›‘ç£çš„é²æ£’è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ
- **Vision Transformer**: Dosovitskiyç­‰äººï¼Œ"ä¸€å¹…å›¾åƒå€¼16x16ä¸ªå•è¯ï¼šåŸºäºè§„æ¨¡çš„å›¾åƒè¯†åˆ«Transformer"
- **PyTorch**: Facebook AIç ”ç©¶æ·±åº¦å­¦ä¹ æ¡†æ¶
- **Hugging Face Transformers**: å¼€æºTransformeræ¶æ„åº“å’Œæ¨¡å‹ä»“åº“

## è”ç³»æ–¹å¼

- **é—®é¢˜**: [GitHub Issues](https://github.com/Nathanielneil/TriModalFusion/issues)
- **è®¨è®º**: [GitHub Discussions](https://github.com/Nathanielneil/TriModalFusion/discussions)
- **é‚®ç®±**: guowei_ni@bit.edu.cn

---

**TriModalFusion** - å¤šæ¨¡æ€æŒ‡ä»¤å¤„ç†åˆ†æçš„ç»Ÿä¸€è®¡ç®—æ¡†æ¶ã€‚
