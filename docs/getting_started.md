# TriModalFusion å¿«é€Ÿå¼€å§‹æŒ‡å—

## ç›®å½•
- [ç³»ç»Ÿè¦æ±‚](#ç³»ç»Ÿè¦æ±‚)
- [å®‰è£…æŒ‡å—](#å®‰è£…æŒ‡å—)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [åŸºç¡€æ¦‚å¿µ](#åŸºç¡€æ¦‚å¿µ)
- [ç¬¬ä¸€ä¸ªç¤ºä¾‹](#ç¬¬ä¸€ä¸ªç¤ºä¾‹)
- [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
- [ä¸‹ä¸€æ­¥](#ä¸‹ä¸€æ­¥)

## ç³»ç»Ÿè¦æ±‚

### ç¡¬ä»¶è¦æ±‚

**æœ€ä½é…ç½®:**
- CPU: Intel i5 æˆ– AMD Ryzen 5 (4æ ¸å¿ƒ)
- å†…å­˜: 8GB RAM
- å­˜å‚¨: 10GB å¯ç”¨ç©ºé—´
- GPU: å¯é€‰ï¼Œæ¨èNVIDIA GTX 1060æˆ–æ›´é«˜

**æ¨èé…ç½®:**
- CPU: Intel i7 æˆ– AMD Ryzen 7 (8æ ¸å¿ƒ)
- å†…å­˜: 16GB RAM
- å­˜å‚¨: 50GB å¯ç”¨ç©ºé—´ (SSDæ¨è)
- GPU: NVIDIA RTX 3070 æˆ–æ›´é«˜ï¼Œ8GB+ æ˜¾å­˜

**ç”Ÿäº§ç¯å¢ƒ:**
- CPU: Intel Xeon æˆ– AMD EPYC (16+æ ¸å¿ƒ)
- å†…å­˜: 32GB+ RAM
- å­˜å‚¨: 100GB+ NVMe SSD
- GPU: NVIDIA A100, V100 æˆ– RTX 4090

### è½¯ä»¶è¦æ±‚

**æ“ä½œç³»ç»Ÿ:**
- Ubuntu 18.04+ / CentOS 7+
- macOS 10.15+
- Windows 10+

**Pythonç¯å¢ƒ:**
- Python 3.8-3.11
- pip 21.0+
- conda (æ¨è)

**æ ¸å¿ƒä¾èµ–:**
- PyTorch 2.0+
- torchvision 0.15+
- torchaudio 2.0+
- CUDA 11.8+ (GPUæ”¯æŒ)

## å®‰è£…æŒ‡å—

### æ–¹æ³•1: Condaç¯å¢ƒ (æ¨è)

```bash
# 1. åˆ›å»ºcondaç¯å¢ƒ
conda create -n trimodal python=3.9
conda activate trimodal

# 2. å®‰è£…PyTorch (CUDAç‰ˆæœ¬)
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# æˆ–è€…å®‰è£…CPUç‰ˆæœ¬
# conda install pytorch torchvision torchaudio cpuonly -c pytorch

# 3. å…‹éš†é¡¹ç›®
git clone https://github.com/your-org/TriModalFusion.git
cd TriModalFusion

# 4. å®‰è£…é¡¹ç›®ä¾èµ–
pip install -r requirements.txt

# 5. å®‰è£…é¡¹ç›®åŒ… (å¼€å‘æ¨¡å¼)
pip install -e .
```

### æ–¹æ³•2: Dockerç¯å¢ƒ

```bash
# 1. æ‹‰å–é¢„æ„å»ºé•œåƒ
docker pull trimodal/trimodal-fusion:latest

# æˆ–è€…æ„å»ºæœ¬åœ°é•œåƒ
docker build -t trimodal-fusion .

# 2. è¿è¡Œå®¹å™¨
docker run -it --gpus all -v $(pwd):/workspace trimodal-fusion:latest

# 3. åœ¨å®¹å™¨ä¸­è¿è¡Œ
cd /workspace
python examples/basic_usage.py
```

### æ–¹æ³•3: pipå®‰è£…

```bash
# 1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv trimodal_env
source trimodal_env/bin/activate  # Linux/Mac
# trimodal_env\Scripts\activate    # Windows

# 2. å‡çº§pip
pip install --upgrade pip

# 3. å®‰è£…PyTorch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# 4. å…‹éš†å¹¶å®‰è£…é¡¹ç›®
git clone https://github.com/your-org/TriModalFusion.git
cd TriModalFusion
pip install -r requirements.txt
pip install -e .
```

### éªŒè¯å®‰è£…

```python
# test_installation.py
import torch
import sys
print(f"Pythonç‰ˆæœ¬: {sys.version}")
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAæ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}")

# æµ‹è¯•å¯¼å…¥
try:
    from src.models.trimodal_fusion import TriModalFusionModel
    print("âœ“ TriModalFusionå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âœ— å¯¼å…¥å¤±è´¥: {e}")

# æµ‹è¯•MediaPipe
try:
    import mediapipe as mp
    print("âœ“ MediaPipeå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âœ— MediaPipeå¯¼å…¥å¤±è´¥: {e}")

# æµ‹è¯•åŸºç¡€åŠŸèƒ½
try:
    from src.utils.config import load_config
    config = load_config("configs/default_config.yaml")
    model = TriModalFusionModel(config)
    print(f"âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {model.get_num_parameters():,}")
except Exception as e:
    print(f"âœ— æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
```

## å¿«é€Ÿå¼€å§‹

### 5åˆ†é’Ÿå¿«é€Ÿä½“éªŒ

```python
# quick_start.py
import torch
from src.models.trimodal_fusion import TriModalFusionModel
from src.utils.config import load_config

# 1. åŠ è½½é…ç½®
print("1. åŠ è½½é…ç½®...")
config = load_config("configs/default_config.yaml")

# 2. åˆ›å»ºæ¨¡å‹
print("2. åˆ›å»ºæ¨¡å‹...")
model = TriModalFusionModel(config)
print(f"   æ¨¡å‹å‚æ•°æ•°é‡: {model.get_num_parameters():,}")

# 3. å‡†å¤‡ç¤ºä¾‹æ•°æ®
print("3. å‡†å¤‡ç¤ºä¾‹æ•°æ®...")
batch_size = 2
inputs = {
    'speech': torch.randn(batch_size, 16000),        # 1ç§’éŸ³é¢‘ @16kHz
    'gesture': torch.randn(batch_size, 30, 2, 21, 3), # 30å¸§æ‰‹åŠ¿ï¼ŒåŒæ‰‹21ä¸ªå…³é”®ç‚¹
    'image': torch.randn(batch_size, 3, 224, 224)    # 224x224 RGBå›¾åƒ
}

# 4. è¿è¡Œæ¨ç†
print("4. è¿è¡Œæ¨ç†...")
model.eval()
with torch.no_grad():
    outputs = model(inputs)

# 5. æŸ¥çœ‹ç»“æœ
print("5. è¾“å‡ºç»“æœ:")
print(f"   - åˆ†ç±»logitså½¢çŠ¶: {outputs['task_outputs']['classification'].shape}")
print(f"   - èåˆç‰¹å¾å½¢çŠ¶: {outputs['fused_features'].shape}")
print("âœ“ å¿«é€Ÿä½“éªŒå®Œæˆ!")
```

### ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹

```python
# pretrained_example.py
import torch
from src.models.trimodal_fusion import TriModalFusionModel
from src.utils.config import load_config

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
def load_pretrained_model(checkpoint_path):
    """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
    config = load_config("configs/pretrained_config.yaml")
    model = TriModalFusionModel(config)
    
    # åŠ è½½æƒé‡
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    model.load_state_dict(checkpoint['model_state_dict'])
    
    return model, config

# ä½¿ç”¨ç¤ºä¾‹
model, config = load_pretrained_model("checkpoints/pretrained_model.pth")

# å‡†å¤‡çœŸå®æ•°æ® (æ›¿æ¢ä¸ºæ‚¨çš„æ•°æ®)
inputs = {
    'speech': torch.randn(1, 16000),
    'gesture': torch.randn(1, 30, 2, 21, 3),
    'image': torch.randn(1, 3, 224, 224)
}

# æ¨ç†
model.eval()
with torch.no_grad():
    outputs = model(inputs)
    predictions = torch.softmax(outputs['task_outputs']['classification'], dim=-1)
    predicted_class = torch.argmax(predictions, dim=-1)

print(f"é¢„æµ‹ç±»åˆ«: {predicted_class.item()}")
print(f"ç½®ä¿¡åº¦: {predictions.max().item():.4f}")
```

## åŸºç¡€æ¦‚å¿µ

### æ ¸å¿ƒç»„ä»¶

**1. å¤šæ¨¡æ€ç¼–ç å™¨**
```
TriModalFusion åŒ…å«ä¸‰ä¸ªä¸“é—¨çš„ç¼–ç å™¨:
â”œâ”€â”€ SpeechEncoder: å¤„ç†éŸ³é¢‘ä¿¡å·
â”‚   â””â”€â”€ åŸºäºTransformerçš„è¯­éŸ³ç‰¹å¾æå–
â”œâ”€â”€ GestureEncoder: å¤„ç†æ‰‹åŠ¿åºåˆ—  
â”‚   â””â”€â”€ MediaPipe + GCN + æ—¶åºCNN
â””â”€â”€ ImageEncoder: å¤„ç†å›¾åƒæ•°æ®
    â””â”€â”€ Vision Transformer æˆ– CNN
```

**2. ç‰¹å¾èåˆ**
```
èåˆæœºåˆ¶å¤„ç†è·¨æ¨¡æ€ä¿¡æ¯æ•´åˆ:
â”œâ”€â”€ æ—¶åºå¯¹é½: åŒæ­¥ä¸åŒæ¨¡æ€çš„æ—¶é—´ä¿¡æ¯
â”œâ”€â”€ è¯­ä¹‰å¯¹é½: å°†ç‰¹å¾æ˜ å°„åˆ°å…±åŒè¯­ä¹‰ç©ºé—´
â””â”€â”€ è·¨æ¨¡æ€æ³¨æ„åŠ›: å­¦ä¹ æ¨¡æ€é—´çš„ç›¸äº’å…³ç³»
```

**3. å¤šä»»åŠ¡è¾“å‡º**
```
æ”¯æŒå¤šç§ä¸‹æ¸¸ä»»åŠ¡:
â”œâ”€â”€ åˆ†ç±»ä»»åŠ¡: å¤šç±»åˆ«åˆ†ç±»
â”œâ”€â”€ æ£€æµ‹ä»»åŠ¡: ç›®æ ‡æ£€æµ‹ (å¯é€‰)
â”œâ”€â”€ å›å½’ä»»åŠ¡: æ•°å€¼é¢„æµ‹ (å¯é€‰)
â””â”€â”€ ç”Ÿæˆä»»åŠ¡: åºåˆ—ç”Ÿæˆ (å¯é€‰)
```

### æ•°æ®æ ¼å¼

**è¾“å…¥æ•°æ®æ ¼å¼:**
```python
inputs = {
    'speech': torch.Tensor,    # [B, audio_length]
    'gesture': torch.Tensor,   # [B, time, hands, joints, coords]
    'image': torch.Tensor      # [B, channels, height, width]
}

# å…·ä½“å½¢çŠ¶ç¤ºä¾‹:
# speech: [2, 16000] - 2ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ª1ç§’@16kHz
# gesture: [2, 30, 2, 21, 3] - 2ä¸ªæ ·æœ¬ï¼Œ30å¸§ï¼Œ2åªæ‰‹ï¼Œ21ä¸ªå…³èŠ‚ï¼Œ3ä¸ªåæ ‡(x,y,z)
# image: [2, 3, 224, 224] - 2ä¸ªæ ·æœ¬ï¼ŒRGBå›¾åƒ224x224
```

**è¾“å‡ºæ•°æ®æ ¼å¼:**
```python
outputs = {
    'task_outputs': {
        'classification': torch.Tensor,  # [B, num_classes]
        'detection': dict,               # æ£€æµ‹ç»“æœ (å¦‚æœå¯ç”¨)
        'regression': torch.Tensor,      # å›å½’ç»“æœ (å¦‚æœå¯ç”¨)
    },
    'fused_features': torch.Tensor,      # [B, seq_len, d_model] èåˆç‰¹å¾
    'encoded_features': {                # å„æ¨¡æ€ç¼–ç ç‰¹å¾
        'speech': torch.Tensor,
        'gesture': torch.Tensor,
        'image': torch.Tensor
    },
    'attention_weights': dict            # æ³¨æ„åŠ›æƒé‡ (å¯é€‰)
}
```

## ç¬¬ä¸€ä¸ªç¤ºä¾‹

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹æ¥äº†è§£æ•´ä¸ªæµç¨‹:

### ç¤ºä¾‹1: åŸºç¡€åˆ†ç±»ä»»åŠ¡

```python
# example_classification.py
import torch
import torch.nn.functional as F
from src.models.trimodal_fusion import TriModalFusionModel
from src.utils.config import load_config
from src.utils.logging_utils import setup_logging

# è®¾ç½®æ—¥å¿—
setup_logging()

def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
    batch_size = 4
    
    # æ¨¡æ‹ŸçœŸå®æ•°æ®çš„åˆ†å¸ƒ
    inputs = {
        'speech': torch.randn(batch_size, 16000) * 0.5,  # éŸ³é¢‘æ•°æ®
        'gesture': torch.randn(batch_size, 30, 2, 21, 3) * 0.1,  # æ‰‹åŠ¿å…³é”®ç‚¹
        'image': torch.randn(batch_size, 3, 224, 224) * 2 - 1  # å›¾åƒæ•°æ® [-1, 1]
    }
    
    # æ¨¡æ‹Ÿåˆ†ç±»æ ‡ç­¾
    targets = torch.randint(0, 10, (batch_size,))
    
    return inputs, targets

def main():
    print("=== TriModalFusion åˆ†ç±»ç¤ºä¾‹ ===\n")
    
    # 1. åŠ è½½é…ç½®
    print("1. åŠ è½½é…ç½®...")
    config = load_config("configs/default_config.yaml")
    print(f"   é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸï¼Œä»»åŠ¡: {config.model.tasks}")
    
    # 2. åˆ›å»ºæ¨¡å‹
    print("\n2. åˆ›å»ºæ¨¡å‹...")
    model = TriModalFusionModel(config)
    print(f"   æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"   - æ€»å‚æ•°æ•°é‡: {model.get_num_parameters():,}")
    print(f"   - æ¨¡å‹å¤§å°: {model.get_num_parameters() * 4 / 1024**2:.1f} MB")
    
    # 3. å‡†å¤‡æ•°æ®
    print("\n3. å‡†å¤‡æ•°æ®...")
    inputs, targets = create_sample_data()
    print(f"   - è¯­éŸ³è¾“å…¥: {inputs['speech'].shape}")
    print(f"   - æ‰‹åŠ¿è¾“å…¥: {inputs['gesture'].shape}")
    print(f"   - å›¾åƒè¾“å…¥: {inputs['image'].shape}")
    print(f"   - ç›®æ ‡æ ‡ç­¾: {targets.shape}")
    
    # 4. å‰å‘ä¼ æ’­
    print("\n4. å‰å‘ä¼ æ’­...")
    model.eval()
    with torch.no_grad():
        outputs = model(inputs)
    
    print("   å‰å‘ä¼ æ’­å®Œæˆ!")
    print(f"   - åˆ†ç±»è¾“å‡ºå½¢çŠ¶: {outputs['task_outputs']['classification'].shape}")
    print(f"   - èåˆç‰¹å¾å½¢çŠ¶: {outputs['fused_features'].shape}")
    
    # 5. é¢„æµ‹ç»“æœ
    print("\n5. é¢„æµ‹ç»“æœ...")
    logits = outputs['task_outputs']['classification']
    probabilities = F.softmax(logits, dim=-1)
    predictions = torch.argmax(logits, dim=-1)
    
    print("   æ ·æœ¬é¢„æµ‹ç»“æœ:")
    for i in range(len(targets)):
        true_label = targets[i].item()
        pred_label = predictions[i].item()
        confidence = probabilities[i, pred_label].item()
        
        status = "âœ“" if pred_label == true_label else "âœ—"
        print(f"   {status} æ ·æœ¬{i+1}: çœŸå®={true_label}, é¢„æµ‹={pred_label}, ç½®ä¿¡åº¦={confidence:.3f}")
    
    # 6. ç‰¹å¾åˆ†æ
    print("\n6. ç‰¹å¾åˆ†æ...")
    if 'encoded_features' in outputs:
        for modality, features in outputs['encoded_features'].items():
            print(f"   - {modality}ç¼–ç ç‰¹å¾: {features.shape}")
    
    # 7. æ¨¡æ€è´¡çŒ®åº¦åˆ†æ (ç®€åŒ–ç‰ˆ)
    print("\n7. æ¨¡æ€é‡è¦æ€§åˆ†æ...")
    baseline_logits = outputs['task_outputs']['classification']
    baseline_confidence = F.softmax(baseline_logits, dim=-1).max(dim=-1)[0].mean()
    
    print(f"   - å®Œæ•´æ¨¡å‹ç½®ä¿¡åº¦: {baseline_confidence:.4f}")
    
    # æµ‹è¯•å•æ¨¡æ€æ€§èƒ½
    for modality in ['speech', 'gesture', 'image']:
        single_input = {modality: inputs[modality]}
        with torch.no_grad():
            single_output = model(single_input)
            single_logits = single_output['task_outputs']['classification']
            single_confidence = F.softmax(single_logits, dim=-1).max(dim=-1)[0].mean()
        
        print(f"   - ä»…{modality}ç½®ä¿¡åº¦: {single_confidence:.4f}")
    
    print("\nâœ“ ç¤ºä¾‹è¿è¡Œå®Œæˆ!")

if __name__ == "__main__":
    main()
```

### ç¤ºä¾‹2: è®­ç»ƒç®€å•æ¨¡å‹

```python
# example_training.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from src.models.trimodal_fusion import TriModalFusionModel
from src.utils.config import load_config

def create_synthetic_dataset(num_samples=1000):
    """åˆ›å»ºåˆæˆæ•°æ®é›†"""
    # åˆ›å»ºè¾“å…¥æ•°æ®
    speech_data = torch.randn(num_samples, 16000) * 0.5
    gesture_data = torch.randn(num_samples, 30, 2, 21, 3) * 0.1
    image_data = torch.randn(num_samples, 3, 224, 224) * 2 - 1
    
    # åˆ›å»ºæ ‡ç­¾ (åŸºäºæ•°æ®çš„æŸäº›ç®€å•è§„å¾‹)
    labels = ((speech_data.mean(dim=1) + 
               gesture_data.mean(dim=(1,2,3,4)) + 
               image_data.mean(dim=(1,2,3))) > 0).long()
    
    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
    inputs_list = []
    for i in range(num_samples):
        inputs_list.append({
            'speech': speech_data[i],
            'gesture': gesture_data[i],
            'image': image_data[i]
        })
    
    return inputs_list, labels

def collate_fn(batch):
    """è‡ªå®šä¹‰æ•°æ®æ•´ç†å‡½æ•°"""
    inputs_batch = {}
    targets_batch = []
    
    for inputs, target in batch:
        targets_batch.append(target)
        for modality, data in inputs.items():
            if modality not in inputs_batch:
                inputs_batch[modality] = []
            inputs_batch[modality].append(data)
    
    # å †å æ•°æ®
    for modality in inputs_batch:
        inputs_batch[modality] = torch.stack(inputs_batch[modality])
    
    targets_batch = torch.stack(targets_batch)
    
    return inputs_batch, targets_batch

def train_simple_model():
    """è®­ç»ƒç®€å•æ¨¡å‹ç¤ºä¾‹"""
    print("=== TriModalFusion è®­ç»ƒç¤ºä¾‹ ===\n")
    
    # 1. å‡†å¤‡æ•°æ®
    print("1. å‡†å¤‡æ•°æ®...")
    inputs_list, labels = create_synthetic_dataset(num_samples=200)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int(0.8 * len(inputs_list))
    train_inputs = inputs_list[:train_size]
    train_labels = labels[:train_size]
    val_inputs = inputs_list[train_size:]
    val_labels = labels[train_size:]
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = list(zip(train_inputs, train_labels))
    val_dataset = list(zip(val_inputs, val_labels))
    
    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)
    
    print(f"   è®­ç»ƒé›†: {len(train_dataset)}ä¸ªæ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(val_dataset)}ä¸ªæ ·æœ¬")
    
    # 2. åˆ›å»ºæ¨¡å‹
    print("\n2. åˆ›å»ºæ¨¡å‹...")
    config = load_config("configs/default_config.yaml")
    # è°ƒæ•´ä¸ºäºŒåˆ†ç±»
    config.model.num_classes = 2
    
    model = TriModalFusionModel(config)
    print(f"   æ¨¡å‹å‚æ•°æ•°é‡: {model.get_num_parameters():,}")
    
    # 3. è®¾ç½®è®­ç»ƒ
    print("\n3. è®¾ç½®è®­ç»ƒ...")
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.CrossEntropyLoss()
    
    # 4. è®­ç»ƒå¾ªç¯
    print("\n4. å¼€å§‹è®­ç»ƒ...")
    num_epochs = 5
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        
        for batch_idx, (inputs, targets) in enumerate(train_loader):
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            outputs = model(inputs)
            logits = outputs['task_outputs']['classification']
            
            # è®¡ç®—æŸå¤±
            loss = criterion(logits, targets)
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            # ç»Ÿè®¡
            train_loss += loss.item()
            _, predicted = torch.max(logits, 1)
            train_total += targets.size(0)
            train_correct += (predicted == targets).sum().item()
            
            if batch_idx % 5 == 0:
                print(f"   Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}")
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for inputs, targets in val_loader:
                outputs = model(inputs)
                logits = outputs['task_outputs']['classification']
                loss = criterion(logits, targets)
                
                val_loss += loss.item()
                _, predicted = torch.max(logits, 1)
                val_total += targets.size(0)
                val_correct += (predicted == targets).sum().item()
        
        # è¾“å‡ºç»“æœ
        train_acc = 100 * train_correct / train_total
        val_acc = 100 * val_correct / val_total
        
        print(f"\n   Epoch {epoch+1}/{num_epochs}:")
        print(f"   è®­ç»ƒ - Loss: {train_loss/len(train_loader):.4f}, Acc: {train_acc:.2f}%")
        print(f"   éªŒè¯ - Loss: {val_loss/len(val_loader):.4f}, Acc: {val_acc:.2f}%")
        print("-" * 50)
    
    print("\nâœ“ è®­ç»ƒå®Œæˆ!")
    
    # 5. ä¿å­˜æ¨¡å‹
    print("\n5. ä¿å­˜æ¨¡å‹...")
    torch.save({
        'model_state_dict': model.state_dict(),
        'config': config,
        'final_val_acc': val_acc
    }, 'simple_trained_model.pth')
    
    print("   æ¨¡å‹å·²ä¿å­˜è‡³: simple_trained_model.pth")

if __name__ == "__main__":
    train_simple_model()
```

## é…ç½®è¯´æ˜

### åŸºç¡€é…ç½®ç»“æ„

```yaml
# configs/getting_started.yaml
model:
  name: "TriModalFusion"
  d_model: 256                    # è¾ƒå°çš„æ¨¡å‹ç”¨äºå¿«é€Ÿå®éªŒ
  tasks: ["classification"]
  num_classes: 10

# ç®€åŒ–çš„æ¨¡æ€é…ç½®
speech_config:
  sample_rate: 16000
  n_mels: 40                      # å‡å°‘è®¡ç®—é‡
  encoder_layers: 3               # è¾ƒå°‘çš„å±‚æ•°
  encoder_attention_heads: 4

gesture_config:
  num_hands: 2
  spatial_hidden_dim: 32          # è¾ƒå°çš„éšè—å±‚
  temporal_hidden_dim: 64

image_config:
  img_size: 224
  image_architecture: "vit"
  vit_config:
    embed_dim: 256                # è¾ƒå°çš„åµŒå…¥ç»´åº¦
    depth: 6                      # è¾ƒå°‘çš„å±‚æ•°
    num_heads: 4

# ç®€å•çš„èåˆé…ç½®
fusion_config:
  alignment_method: "interpolation"
  fusion_strategy: "attention"
  fusion_heads: 4

# è®­ç»ƒé…ç½®
training:
  batch_size: 16                  # è¾ƒå°çš„æ‰¹æ¬¡å¤§å°
  learning_rate: 1e-4
  max_epochs: 10
  
# ç³»ç»Ÿé…ç½®
system:
  device: "auto"
  precision: 32                   # ä½¿ç”¨float32ç¡®ä¿ç¨³å®šæ€§
```

### é…ç½®æ–‡ä»¶è§£é‡Š

1. **model**: å®šä¹‰æ¨¡å‹çš„åŸºæœ¬æ¶æ„å‚æ•°
2. **speech_config**: è¯­éŸ³æ¨¡æ€çš„ç‰¹å®šé…ç½®
3. **gesture_config**: æ‰‹åŠ¿æ¨¡æ€çš„ç‰¹å®šé…ç½®  
4. **image_config**: å›¾åƒæ¨¡æ€çš„ç‰¹å®šé…ç½®
5. **fusion_config**: å¤šæ¨¡æ€èåˆçš„é…ç½®
6. **training**: è®­ç»ƒç›¸å…³çš„é…ç½®
7. **system**: ç³»ç»Ÿå’Œç¡¬ä»¶ç›¸å…³çš„é…ç½®

## å¸¸è§é—®é¢˜

### Q1: å¯¼å…¥é”™è¯¯ "No module named 'src'"

**è§£å†³æ–¹æ¡ˆ:**
```bash
# ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹
cd TriModalFusion

# å®‰è£…ä¸ºå¯ç¼–è¾‘åŒ…
pip install -e .

# æˆ–è€…æ·»åŠ åˆ°Pythonè·¯å¾„
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
```

### Q2: CUDAå†…å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ:**
```python
# 1. å‡å°æ‰¹æ¬¡å¤§å°
config.training.batch_size = 8  # æˆ–æ›´å°

# 2. å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
config.system.precision = 16

# 3. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
config.system.memory_optimization.gradient_checkpointing = True

# 4. å‡å°æ¨¡å‹å¤§å°
config.model.d_model = 256
config.speech_config.encoder_layers = 3
```

### Q3: MediaPipeå®‰è£…å¤±è´¥

**è§£å†³æ–¹æ¡ˆ:**
```bash
# æ–¹æ³•1: ä½¿ç”¨condaå®‰è£…
conda install -c conda-forge mediapipe

# æ–¹æ³•2: æ›´æ–°pipå¹¶é‡æ–°å®‰è£…
pip install --upgrade pip
pip install mediapipe

# æ–¹æ³•3: å¦‚æœä»ç„¶å¤±è´¥ï¼Œç¦ç”¨MediaPipe
# åœ¨é…ç½®ä¸­è®¾ç½®:
# gesture_config:
#   use_mediapipe: false
```

### Q4: æ¨¡å‹è®­ç»ƒå¾ˆæ…¢

**ä¼˜åŒ–å»ºè®®:**
```python
# 1. ä½¿ç”¨æ›´å°çš„æ•°æ®å°ºå¯¸
config.speech_config.max_audio_length = 1000  # å‡å°‘éŸ³é¢‘é•¿åº¦
config.gesture_config.max_sequence_length = 64  # å‡å°‘æ‰‹åŠ¿åºåˆ—é•¿åº¦
config.image_config.img_size = 128  # å‡å°‘å›¾åƒå°ºå¯¸

# 2. å‡å°‘æ¨¡å‹å¤æ‚åº¦
config.model.d_model = 256
config.fusion_config.fusion_heads = 4

# 3. å¯ç”¨ç¼–è¯‘åŠ é€Ÿ (PyTorch 2.0+)
config.system.compile_model = True
```

### Q5: å¤šGPUè®­ç»ƒå‡ºé”™

**è§£å†³æ–¹æ¡ˆ:**
```bash
# 1. ç¡®ä¿æ‰€æœ‰GPUå¯è§
nvidia-smi

# 2. ä½¿ç”¨æ­£ç¡®çš„å¯åŠ¨æ–¹å¼
python -m torch.distributed.launch --nproc_per_node=2 train.py

# 3. æˆ–ä½¿ç”¨torchrun
torchrun --nproc_per_node=2 train.py --config configs/default_config.yaml
```

### Q6: é¢„æµ‹ç»“æœä¸åˆç†

**æ’æŸ¥æ­¥éª¤:**
```python
# 1. æ£€æŸ¥æ•°æ®é¢„å¤„ç†
print("Input ranges:")
for modality, data in inputs.items():
    print(f"{modality}: min={data.min():.3f}, max={data.max():.3f}, mean={data.mean():.3f}")

# 2. æ£€æŸ¥æ¨¡å‹è¾“å‡º
with torch.no_grad():
    outputs = model(inputs)
    logits = outputs['task_outputs']['classification']
    print(f"Logits range: min={logits.min():.3f}, max={logits.max():.3f}")
    
# 3. æ£€æŸ¥æ¢¯åº¦
model.train()
outputs = model(inputs)
loss = criterion(outputs['task_outputs']['classification'], targets)
loss.backward()

grad_norm = 0
for param in model.parameters():
    if param.grad is not None:
        grad_norm += param.grad.data.norm(2).item() ** 2
grad_norm = grad_norm ** 0.5
print(f"Gradient norm: {grad_norm:.3f}")
```

## ä¸‹ä¸€æ­¥

ç°åœ¨æ‚¨å·²ç»å®Œæˆäº†åŸºç¡€è®¾ç½®ï¼Œå¯ä»¥ç»§ç»­å­¦ä¹ :

1. **[è®­ç»ƒæŒ‡å—](training_guide.md)** - å­¦ä¹ å¦‚ä½•è®­ç»ƒè‡ªå·±çš„æ¨¡å‹
2. **[é…ç½®æŒ‡å—](configuration.md)** - æ·±å…¥äº†è§£é…ç½®é€‰é¡¹
3. **[æ¨¡å‹æ¶æ„](model_architecture.md)** - ç†è§£æ¨¡å‹å†…éƒ¨ç»“æ„
4. **[è¯„ä¼°æŒ‡å—](evaluation.md)** - å­¦ä¹ å¦‚ä½•è¯„ä¼°æ¨¡å‹æ€§èƒ½

### è¿›é˜¶ç¤ºä¾‹

```python
# è¿è¡Œæ›´å¤æ‚çš„ç¤ºä¾‹
python examples/multimodal_classification.py
python examples/attention_visualization.py
python examples/feature_extraction.py
```

### è‡ªå®šä¹‰æ•°æ®é›†

```python
# å‡†å¤‡æ‚¨è‡ªå·±çš„æ•°æ®
from src.data.custom_dataset import MultiModalDataset
from src.data.preprocessor import MultiModalPreprocessor

# é¢„å¤„ç†æ•°æ®
preprocessor = MultiModalPreprocessor(config)
processed_data = preprocessor.process_directory("path/to/your/data")

# åˆ›å»ºæ•°æ®é›†
dataset = MultiModalDataset(processed_data, config)
```

### ç¤¾åŒºå’Œæ”¯æŒ

- **GitHub Issues**: [æŠ¥å‘Šé—®é¢˜](https://github.com/your-org/TriModalFusion/issues)
- **GitHub Discussions**: [ç¤¾åŒºè®¨è®º](https://github.com/your-org/TriModalFusion/discussions)
- **æ–‡æ¡£**: [å®Œæ•´æ–‡æ¡£](https://trimodal-fusion.readthedocs.io/)
- **ç¤ºä¾‹**: [examples/](../examples/) ç›®å½•

ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼ğŸš€