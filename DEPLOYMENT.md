# TriModalFusion éƒ¨ç½²æŒ‡å—

## éƒ¨ç½²æ¦‚è¿°

TriModalFusionæä¾›äº†å¤šç§éƒ¨ç½²æ–¹å¼ï¼Œä»Žå¼€å‘æµ‹è¯•åˆ°ç”Ÿäº§çŽ¯å¢ƒçš„å®Œæ•´è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº†å„ç§éƒ¨ç½²é€‰é¡¹ã€é…ç½®å’Œæœ€ä½³å®žè·µã€‚

## ðŸš€ å¿«é€Ÿéƒ¨ç½²

### 1. æœ¬åœ°å¼€å‘éƒ¨ç½²

```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/your-org/TriModalFusion.git
cd TriModalFusion

# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt
pip install -r requirements-prod.txt

# 3. ä¸‹è½½é¢„è®­ç»ƒæ¨¡åž‹
mkdir -p models
# å°†è®­ç»ƒå¥½çš„æ¨¡åž‹æ”¾åˆ° models/best_model.pth

# 4. å¯åŠ¨APIæœåŠ¡
python deployment/serve.py --host 0.0.0.0 --port 8000
```

è®¿é—® `http://localhost:8000/docs` æŸ¥çœ‹APIæ–‡æ¡£ã€‚

### 2. Docker å•å®¹å™¨éƒ¨ç½²

```bash
# æž„å»ºDockeré•œåƒ
docker build -t trimodal-fusion -f deployment/docker/Dockerfile .

# è¿è¡Œå®¹å™¨
docker run -d \
  --name trimodal-api \
  --gpus all \
  -p 8000:8000 \
  -v $(pwd)/models:/app/models:ro \
  -v $(pwd)/configs:/app/configs:ro \
  trimodal-fusion
```

## ðŸ¢ ç”Ÿäº§çŽ¯å¢ƒéƒ¨ç½²

### Docker Compose éƒ¨ç½²ï¼ˆæŽ¨èï¼‰

å®Œæ•´çš„ç”Ÿäº§çº§éƒ¨ç½²ï¼ŒåŒ…å«è´Ÿè½½å‡è¡¡ã€ç¼“å­˜ã€æ•°æ®åº“å’Œç›‘æŽ§ã€‚

```bash
cd deployment/docker

# å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f trimodal-api

# æ‰©å±•APIæœåŠ¡
docker-compose up -d --scale trimodal-api=3
```

#### æœåŠ¡æž¶æž„ï¼š
- **trimodal-api**: ä¸»APIæœåŠ¡
- **nginx**: è´Ÿè½½å‡è¡¡å’Œåå‘ä»£ç†
- **redis**: ç¼“å­˜å’Œä¼šè¯å­˜å‚¨
- **postgres**: æ•°æ®åº“
- **prometheus + grafana**: ç›‘æŽ§ç³»ç»Ÿ
- **elasticsearch**: æ—¥å¿—èšåˆ
- **celery**: å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—

#### è®¿é—®åœ°å€ï¼š
- APIæœåŠ¡: `http://localhost`
- APIæ–‡æ¡£: `http://localhost/docs`
- ç›‘æŽ§é¢æ¿: `http://localhost:3000` (admin/admin)
- ä»»åŠ¡ç›‘æŽ§: `http://localhost:5555`
- æŒ‡æ ‡ç›‘æŽ§: `http://localhost:9090`

### Kubernetes éƒ¨ç½²ï¼ˆä¼ä¸šçº§ï¼‰

é€‚ç”¨äºŽå¤§è§„æ¨¡ç”Ÿäº§çŽ¯å¢ƒçš„å®¹å™¨ç¼–æŽ’éƒ¨ç½²ã€‚

```bash
# 1. åˆ›å»ºå‘½åç©ºé—´
kubectl create namespace trimodal

# 2. éƒ¨ç½²åº”ç”¨
kubectl apply -f deployment/kubernetes/

# 3. æ£€æŸ¥éƒ¨ç½²çŠ¶æ€
kubectl get pods -n trimodal
kubectl get services -n trimodal

# 4. æŸ¥çœ‹æ—¥å¿—
kubectl logs -f deployment/trimodal-api -n trimodal

# 5. ç«¯å£è½¬å‘ï¼ˆå¼€å‘æµ‹è¯•ï¼‰
kubectl port-forward service/trimodal-api-service 8000:80 -n trimodal
```

#### åŠŸèƒ½ç‰¹æ€§ï¼š
- **è‡ªåŠ¨æ‰©ç¼©å®¹**: åŸºäºŽCPU/å†…å­˜ä½¿ç”¨çŽ‡
- **æ»šåŠ¨æ›´æ–°**: é›¶åœæœºæ›´æ–°
- **å¥åº·æ£€æŸ¥**: è‡ªåŠ¨é‡å¯æ•…éšœå®¹å™¨
- **èµ„æºé™åˆ¶**: GPUå’Œå†…å­˜èµ„æºç®¡ç†
- **è´Ÿè½½å‡è¡¡**: IngressæŽ§åˆ¶å™¨
- **SSLç»ˆç«¯**: è‡ªåŠ¨HTTPSè¯ä¹¦

## âš™ï¸ é…ç½®ç®¡ç†

### çŽ¯å¢ƒé…ç½®

åˆ›å»º `configs/production_config.yaml`:

```yaml
# æ¨¡åž‹é…ç½®
model:
  name: "TriModalFusion"
  d_model: 512
  tasks: ["classification"]
  num_classes: 10

# æœåŠ¡é…ç½®
serving:
  device: "cuda"
  batch_size: 16
  half_precision: true
  model_path: "/app/models/best_model.pth"
  max_concurrent_requests: 100
  request_timeout: 30

# æ€§èƒ½ä¼˜åŒ–
optimization:
  enable_tensorrt: true
  enable_model_compilation: true
  memory_pool_size: "2GB"
  
# ç›‘æŽ§é…ç½®
monitoring:
  enable_metrics: true
  log_level: "INFO"
  enable_request_logging: true
```

### çŽ¯å¢ƒå˜é‡

```bash
# æ¨¡åž‹é…ç½®
export TRIMODAL_MODEL_PATH="/path/to/best_model.pth"
export TRIMODAL_CONFIG_PATH="/path/to/config.yaml"

# æœåŠ¡é…ç½®
export TRIMODAL_HOST="0.0.0.0"
export TRIMODAL_PORT="8000"
export TRIMODAL_WORKERS="4"

# æ•°æ®åº“è¿žæŽ¥
export DATABASE_URL="postgresql://user:pass@host:5432/trimodal"
export REDIS_URL="redis://host:6379/0"

# å®‰å…¨é…ç½®
export API_KEY="your-secure-api-key"
export JWT_SECRET="your-jwt-secret"

# å­˜å‚¨é…ç½®
export MODEL_STORAGE_PATH="/models"
export LOG_STORAGE_PATH="/logs"
```

## ðŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. æ¨¡åž‹ä¼˜åŒ–

```bash
# è½¬æ¢ä¸ºTensorRTï¼ˆNVIDIA GPUï¼‰
python scripts/optimize_model.py \
  --input-model models/best_model.pth \
  --output-model models/optimized_model.trt \
  --precision fp16

# è½¬æ¢ä¸ºONNXï¼ˆé€šç”¨åŠ é€Ÿï¼‰
python scripts/convert_to_onnx.py \
  --input-model models/best_model.pth \
  --output-model models/model.onnx \
  --dynamic-batch
```

### 2. æœåŠ¡ä¼˜åŒ–

```python
# å¯ç”¨å¹¶å‘å¤„ç†
uvicorn deployment.serve:app \
  --host 0.0.0.0 \
  --port 8000 \
  --workers 4 \
  --worker-class uvicorn.workers.UvicornWorker

# å¯ç”¨å¼‚æ­¥å¤„ç†
gunicorn deployment.serve:app \
  -k uvicorn.workers.UvicornWorker \
  --workers 4 \
  --bind 0.0.0.0:8000 \
  --worker-connections 1000
```

### 3. ç¼“å­˜é…ç½®

```yaml
# Redisç¼“å­˜é…ç½®
cache:
  redis_url: "redis://localhost:6379/0"
  ttl: 3600  # 1å°æ—¶
  enable_result_cache: true
  enable_model_cache: true
```

## ðŸ”’ å®‰å…¨é…ç½®

### 1. APIå®‰å…¨

```python
# APIå¯†é’¥è®¤è¯
@app.middleware("http")
async def verify_api_key(request: Request, call_next):
    api_key = request.headers.get("X-API-Key")
    if not api_key or api_key != os.getenv("API_KEY"):
        return JSONResponse(
            status_code=401,
            content={"detail": "Invalid API key"}
        )
    return await call_next(request)
```

### 2. ç½‘ç»œå®‰å…¨

```nginx
# Nginxå®‰å…¨é…ç½®
server {
    listen 443 ssl http2;
    server_name api.trimodal.example.com;
    
    # SSLé…ç½®
    ssl_certificate /etc/nginx/ssl/cert.pem;
    ssl_certificate_key /etc/nginx/ssl/key.pem;
    
    # å®‰å…¨å¤´
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    
    # é™æµ
    limit_req zone=api burst=20 nodelay;
    
    location / {
        proxy_pass http://trimodal-backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

### 3. å®¹å™¨å®‰å…¨

```dockerfile
# ä½¿ç”¨éžrootç”¨æˆ·
RUN useradd -m -u 1000 trimodal && \
    chown -R trimodal:trimodal /app
USER trimodal

# åªå®‰è£…å¿…è¦åŒ…
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3 \
    && rm -rf /var/lib/apt/lists/*
```

## ðŸ“ˆ ç›‘æŽ§ä¸Žæ—¥å¿—

### 1. åº”ç”¨ç›‘æŽ§

```python
# PrometheusæŒ‡æ ‡
from prometheus_client import Counter, Histogram, Gauge

REQUEST_COUNT = Counter('requests_total', 'Total requests')
REQUEST_LATENCY = Histogram('request_duration_seconds', 'Request latency')
MODEL_ACCURACY = Gauge('model_accuracy', 'Current model accuracy')

@app.middleware("http")
async def monitor_requests(request: Request, call_next):
    start_time = time.time()
    REQUEST_COUNT.inc()
    
    response = await call_next(request)
    
    REQUEST_LATENCY.observe(time.time() - start_time)
    return response
```

### 2. æ—¥å¿—é…ç½®

```python
# ç»“æž„åŒ–æ—¥å¿—
import structlog

logger = structlog.get_logger()

logger.info(
    "Prediction completed",
    request_id=request_id,
    inference_time=inference_time,
    model_version="1.0",
    input_modalities=["speech", "image"]
)
```

### 3. å¥åº·æ£€æŸ¥

```python
@app.get("/health/deep")
async def deep_health_check():
    checks = {
        "model_loaded": predictor is not None,
        "gpu_available": torch.cuda.is_available(),
        "redis_connected": await check_redis_connection(),
        "database_connected": await check_db_connection(),
        "disk_space_ok": check_disk_space() > 0.1,  # >10% å‰©ä½™
        "memory_ok": check_memory_usage() < 0.9,    # <90% ä½¿ç”¨
    }
    
    status = "healthy" if all(checks.values()) else "unhealthy"
    
    return {
        "status": status,
        "checks": checks,
        "timestamp": time.time()
    }
```

## ðŸ”§ æ•…éšœæŽ’é™¤

### å¸¸è§é—®é¢˜

#### 1. CUDAå†…å­˜ä¸è¶³
```bash
# è§£å†³æ–¹æ¡ˆï¼š
export CUDA_VISIBLE_DEVICES=0  # åªä½¿ç”¨ä¸€ä¸ªGPU
# æˆ–åœ¨é…ç½®ä¸­å‡å°‘batch_size
```

#### 2. æ¨¡åž‹åŠ è½½å¤±è´¥
```bash
# æ£€æŸ¥æ¨¡åž‹æ–‡ä»¶
ls -la models/best_model.pth
# æ£€æŸ¥æ¨¡åž‹å…¼å®¹æ€§
python -c "import torch; torch.load('models/best_model.pth', map_location='cpu')"
```

#### 3. APIå“åº”æ…¢
```bash
# æ£€æŸ¥ç³»ç»Ÿèµ„æº
htop
nvidia-smi
# å¯ç”¨æ€§èƒ½åˆ†æž
python -m cProfile deployment/serve.py
```

#### 4. å®¹å™¨å¯åŠ¨å¤±è´¥
```bash
# æŸ¥çœ‹å®¹å™¨æ—¥å¿—
docker logs trimodal-api
# è¿›å…¥å®¹å™¨è°ƒè¯•
docker exec -it trimodal-api bash
```

### æ€§èƒ½åŸºå‡†

| é…ç½® | åžåé‡ (req/s) | å¹³å‡å»¶è¿Ÿ (ms) | GPUå†…å­˜ (GB) |
|------|----------------|---------------|--------------|
| T4 + FP16 | 15 | 150 | 4.2 |
| V100 + FP16 | 45 | 60 | 6.8 |
| A100 + FP16 | 80 | 35 | 12.1 |

### æ‰©å±•å»ºè®®

#### 1. æ°´å¹³æ‰©å±•
```bash
# Docker Composeæ‰©å±•
docker-compose up -d --scale trimodal-api=5

# Kubernetesæ‰©å±•
kubectl scale deployment trimodal-api --replicas=5
```

#### 2. åž‚ç›´æ‰©å±•
```yaml
# å¢žåŠ èµ„æºé™åˆ¶
resources:
  requests:
    memory: "8Gi"
    cpu: "2000m"
    nvidia.com/gpu: 2
  limits:
    memory: "16Gi" 
    cpu: "4000m"
    nvidia.com/gpu: 2
```

## ðŸ“š æ›´å¤šèµ„æº

- [APIæ–‡æ¡£](http://localhost:8000/docs)
- [ç›‘æŽ§é¢æ¿](http://localhost:3000)
- [æ€§èƒ½åŸºå‡†æµ‹è¯•](./benchmark.md)
- [å®‰å…¨æœ€ä½³å®žè·µ](./security.md)
- [æ•…éšœæŽ’é™¤æŒ‡å—](./troubleshooting.md)

---

å¦‚éœ€æŠ€æœ¯æ”¯æŒï¼Œè¯·è®¿é—®ï¼š
- GitHub Issues: https://github.com/your-org/TriModalFusion/issues
- æŠ€æœ¯æ–‡æ¡£: https://trimodal-docs.example.com
- ç¤¾åŒºè®¨è®º: https://discord.gg/trimodal